from table import DBTable
import pandas as pd
from simulation import *
import math
import time
import matplotlib.pyplot as plt
import numpy as np
from typing import Tuple


def plot(x, ys, labels, saveas, x_label, y_label, title):
    plt.figure(figsize=(8, 5))
    for y, li in zip(ys, labels):
        plt.plot(x, y, label=li)
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.title(title)
    plt.legend()
    plt.grid(True)
    plt.savefig(saveas)
    plt.show()



def test_sizes(sizes, dir, filename: str, min_insertion_size = 1000):
    df = pd.read_csv(dir + filename + '.csv')
    df = df.sample(frac=1)
    hash_size = 10
    
    title = "Analysis Of Insertion onto Varying Initial Sized Table"
    #last 1000 are insertion
    n = len(df)
    initialization_times = []
    insertion_times = []
    init_accuracy = []
    insert_accuracy = []
    for size in sizes:
        insert_df = df.iloc[n - min_insertion_size:]
        initial_n = int(math.ceil(n - min_insertion_size) * size)
        initial_df = df.iloc[0:initial_n]
        #evaluating start up time
        start = time.time()
        db_table = DBTable(file_name=None, from_data=initial_df, sort_key='sid', index_levels=[1, 4, 8], hash_size=hash_size)
        end = time.time()
        initialization_times.append(end - start)
        #evaluation insertion time
        start = time.time()
        test_n = len(insert_df)
        for i in range(test_n):
            row = insert_df.iloc[i]
            db_table.insert(row)
        end = time.time()
        insertion_times.append(end - start)
        #evaluation accuracy
        accessible = test_accuracy(initial_df=initial_df, sort_key='sid', table = db_table)
        init_accuracy.append(accessible/len(initial_df))
        accessible = test_accuracy(initial_df=insert_df, sort_key='sid', table = db_table)
        insert_accuracy.append(accessible/(len(insert_df)))
    print(initialization_times, insertion_times, init_accuracy, insert_accuracy)
    plot(sizes, [initialization_times, insertion_times], ["Initialization", "Insertion"], x_label="sizes", y_label="RunTime", title=title, saveas='results/sizes_runtime.png')
    plot(sizes, [init_accuracy, insert_accuracy], ["Initial Data", "Inserted Data"], x_label="sizes", y_label="% of Time", title="Ability to Access Final Data Within Error Bounds", saveas='results/sizes_accuracies.png')
    
def test_depths(depths, dir, filename: str, min_insertion_size = 1000):
    df = pd.read_csv(dir + filename + '.csv')
    df = df.sample(frac=1)
    hash_size = 10
    
    title = "Analysis Of RunTimes with a Varying Depth Limit on Extensible Hashes"
    #last 1000 are insertion
    n = len(df)
    initialization_times = []
    insertion_times = []
    init_accuracy = []
    insert_accuracy = []
    for depth in depths:
        insert_df = df.iloc[n - min_insertion_size:]
        initial_n = int(math.ceil(n - min_insertion_size) * 1)
        initial_df = df.iloc[0:initial_n]
        #evaluating start up time
        start = time.time()
        db_table = DBTable(file_name=None, from_data=initial_df, sort_key='sid', index_levels=[1, 4, 8], hash_size=hash_size, depth_limit=depth)
        end = time.time()
        initialization_times.append(end - start)
        #evaluation insertion time
        start = time.time()
        test_n = len(insert_df)
        for i in range(test_n):
            row = insert_df.iloc[i]
            db_table.insert(row)
        end = time.time()
        insertion_times.append(end - start)
        #evaluation accuracy
        accessible = test_accuracy(initial_df=initial_df, sort_key='sid', table = db_table)
        init_accuracy.append(accessible/len(initial_df))
        accessible = test_accuracy(initial_df=insert_df, sort_key='sid', table = db_table)
        insert_accuracy.append(accessible/(len(insert_df)))
    #since, None represents infinity, for now, graph it as 1000
    depths[len(depths) - 1] = 200
    print(initialization_times, insertion_times, init_accuracy, insert_accuracy)
    plot(depths, [initialization_times, insertion_times], ["Initialization", "Insertion"], x_label="depth", y_label="RunTime", title=title, saveas='results/depth_runtime.png')
    plot(depths, [init_accuracy, insert_accuracy], ["Initial Data", "Inserted Data"], x_label="depth", y_label="% of Time", title="Ability to Access Final Data Within Error Bounds With Changing Depths on Extens. Hashes", saveas='results/depth_accuracies.png')

#generated by ChatGPT
def make_unique_uniform_and_skewed(n1, n2, universe, skew, rng):
    """
    Sample df1 (uniform) and df2 (skewed) with uniqueness across both.
    - n1: size of df1 (uniform)
    - n2: size of df2 (skewed)
    - universe: np.array of integers (support)
    - skew: non-negative float (controls skew). skew == 0 -> uniform.
    - rng: numpy Generator for reproducibility
    Returns: (df1, df2, df_combined) as pandas DataFrames
    """
    # 1) Sample df1 uniformly without replacement from full universe
    df1_values = rng.choice(universe, size=n1, replace=False)

    # 2) Determine remaining pool
    remaining = np.setdiff1d(universe, df1_values, assume_unique=True)

    # 3) Compute skewed probabilities over remaining support
    #    We use power-law style p(x) ∝ 1 / x**skew (x taken as integer magnitude)
    #    If skew == 0, this becomes uniform over remaining.
    if skew == 0:
        probs = np.ones(len(remaining), dtype=float)
    else:
        # use values relative to start (so small integers get higher prob)
        # ensure float computation
        vals = remaining.astype(float)
        probs = 1.0 / (vals ** skew)

    probs /= probs.sum()  # normalize

    # 4) Sample df2 without replacement according to probs
    df2_values = rng.choice(remaining, size=n2, replace=False, p=probs)

    # 5) Build DataFrames (integers guaranteed, unique across dfs)
    df1 = pd.DataFrame({"value": df1_values})
    df2 = pd.DataFrame({"value": df2_values})
    df_combined = pd.concat([df1, df2], ignore_index=True)

    return df1, df2, df_combined
    
def test_skewed_insertion(skews):
    N1 = 10_000          # size of df1 (uniform)
    N2 = 5_000           # size of df2 (skewed inserts)
    max_value = 100000   # integer universe must be >= N1 + N2
    raw_skews = np.linspace(0.0, 2.0, num=9)  # example raw skews
    skews = [round(float(s), 5) for s in raw_skews]  # rounded to 5 decimals
    start = 1            # integer range is [start, max_value]
    rng = np.random.default_rng(seed=42)  # deterministic seed (optional)
    # -------------------------

    # Sanity check
    if max_value - start + 1 < (N1 + N2):
        raise ValueError("The integer universe is too small for unique sampling. "
                        f"Need at least {N1+N2} distinct integers in the range.")

    # integer support
    universe = np.arange(start, max_value + 1)
    hash_size = 10
    
    title = "Analysis Of RunTimes On Insertions Causing Skewed Distribution"
    #last 1000 are insertion
    n = len(df)
    initialization_times = []
    insertion_times = []
    init_accuracy = []
    insert_accuracy = []
    init_select_time = []
    insert_select_time = []
    N1 = 10_000   # size of df1 (uniform)
    N2 = 5_000    # size of df2 (inserted, skewed)
    max_value = 1000  # integer range
    for s in skews:
        #evaluating start up time
        start = time.time()
        df1, insert_df, df_combined_s = make_unique_uniform_and_skewed(
            N1, N2, universe, skew=s, rng=rng)
        db_table = DBTable(file_name=None, from_data=df1, sort_key='value', index_levels=[1, 4, 8], hash_size=hash_size, depth_limit=50)
        end = time.time()
        initialization_times.append(end - start)
        #evaluation insertion time
        start = time.time()
        test_n = len(insert_df)
        for i in range(test_n):
            row = insert_df.iloc[i]
            db_table.insert(row)
        end = time.time()
        insertion_times.append(end - start)
        #evaluation accuracy
        start = time.time()
        accessible = test_accuracy(initial_df=initial_df, sort_key='value', table = db_table)
        end = time.time()
        init_select_time.append((end - start))
        init_accuracy.append(accessible/len(initial_df))
        
        start = time.time()
        accessible = test_accuracy(initial_df=insert_df, sort_key='value', table = db_table)
        end = time.time()
        insert_select_time.append((end - start))
        insert_accuracy.append(accessible/(len(insert_df)))
    #since, None represents infinity, for now, graph it as 1000
    print(initialization_times, insertion_times, init_accuracy, insert_accuracy, init_select_time, insert_select_time)
    plot(skews, [initialization_times, insertion_times], ["Initialization", "Insertion"], x_label="depth", y_label="RunTime", title=title, saveas='results/skew_runtime.png')
    plot(skews, [init_accuracy, insert_accuracy], ["Initial Data", "Inserted Data"], x_label="skew", y_label="% of Time", title="Ability to Access Final Data Within Error Bounds With Changing Skew in Data", saveas='results/skew_accuracies.png')
    plot(skews, [init_select_time, insert_select_time], ["Initial Data", "Inserted Data"], x_label="skew", y_label="RunTime", title="RunTime For Accessing Final Data Within Error Bounds With Changing Skew in Data", saveas='results/skew_select_runtime.png')
      
#NOTE: The following functions were provided by ChatGPT
# ----------------------
# Helper: make datasets
# ----------------------
def make_unique_uniform_and_skewed(n1: int, n2: int, universe: np.ndarray, skew: float, rng: np.random.Generator
                                  ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    Sample df1 (uniform) and df2 (skewed) with uniqueness across both.
    Returns df1, df2, df_combined (all DataFrames with integer 'value' column).
    """
    # 1) Sample df1 uniformly without replacement from full universe
    df1_values = rng.choice(universe, size=n1, replace=False)

    # 2) Determine remaining pool
    remaining = np.setdiff1d(universe, df1_values, assume_unique=True)

    # 3) Compute skewed probabilities over remaining support
    if skew == 0:
        probs = np.ones(len(remaining), dtype=float)
    else:
        vals = remaining.astype(float)
        probs = 1.0 / (vals ** skew)
    probs /= probs.sum()

    # 4) Sample df2 without replacement according to probs
    df2_values = rng.choice(remaining, size=n2, replace=False, p=probs)

    # 5) DataFrames
    df1 = pd.DataFrame({"value": df1_values})
    df2 = pd.DataFrame({"value": df2_values})
    df_combined = pd.concat([df1, df2], ignore_index=True)

    return df1, df2, df_combined

# ----------------------
# Plot helpers
# ----------------------
def plot_hist_and_cdf(values, bins=100, title=None, saveas=None, show=True):
    """
    Plot histogram and CDF (two subplots) for integer values.
    """
    values = np.asarray(values)
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))

    # Histogram
    axes[0].hist(values, bins=bins)
    axes[0].set_title("Histogram" if title is None else f"{title} — Histogram")
    axes[0].set_xlabel("value")
    axes[0].set_ylabel("count")

    # CDF
    sorted_vals = np.sort(values)
    cdf = np.arange(1, len(sorted_vals) + 1) / len(sorted_vals)
    axes[1].plot(sorted_vals, cdf)
    axes[1].set_title("CDF" if title is None else f"{title} — CDF")
    axes[1].set_xlabel("value")
    axes[1].set_ylabel("CDF")

    plt.tight_layout()
    if saveas:
        fig.savefig(saveas)
    if show:
        plt.show()
    plt.close(fig)


def plot_overlay_cdfs(datasets: dict, bins=100, title=None, saveas=None, show=True):
    """
    datasets: dict[label] -> 1D array-like of values
    Plots overlayed CDFs for comparison.
    """
    plt.figure(figsize=(6, 4))
    for label, vals in datasets.items():
        vals = np.sort(np.asarray(vals))
        cdf = np.arange(1, len(vals) + 1) / len(vals)
        plt.plot(vals, cdf, label=str(label))
    plt.xlabel("value")
    plt.ylabel("CDF")
    plt.title("Overlayed CDFs" if title is None else title)
    plt.legend()
    plt.tight_layout()
    if saveas:
        plt.savefig(saveas)
    if show:
        plt.show()
    plt.close()


# ----------------------
# Main test function
# ----------------------
def test_skewed_insertion(skews=None):
    # If caller passes skews, use those; otherwise use default sequence
    if skews is None:
        raw_skews = np.linspace(0.0, 2.0, num=9)
        skews = [round(float(s), 5) for s in raw_skews]

    # Experiment params
    N1 = 10_000          # size of df1 (uniform)
    N2 = 5_000           # size of df2 (skewed inserts)
    max_value = 100000   # integer universe must be >= N1 + N2
    start_val = 1        # integer range is [start_val, max_value]
    rng = np.random.default_rng(seed=42)

    # Sanity check
    if max_value - start_val + 1 < (N1 + N2):
        raise ValueError("The integer universe is too small for unique sampling. "
                         f"Need at least {N1+N2} distinct integers in the range.")

    universe = np.arange(start_val, max_value + 1)
    hash_size = 10

    # Storage for measurements
    title = "Analysis Of RunTimes On Insertions Causing Skewed Distribution"
    initialization_times = []
    insertion_times = []
    init_accuracy = []
    insert_accuracy = []
    init_select_time = []
    insert_select_time = []

    # We'll also keep one combined dataset per skew for plotting later
    combined_by_skew = {}

    # Loop over skew values
    for s in skews:
        # create datasets
        start_ts = time.time()
        df1, insert_df, df_combined_s = make_unique_uniform_and_skewed(
            N1, N2, universe, skew=s, rng=rng)
        end_ts = time.time()
        initialization_times.append(end_ts - start_ts)

        # keep combined for later visualization
        combined_by_skew[s] = df_combined_s["value"].values

        # If you want initial_df to be the original df1 for accuracy tests:
        initial_df = df1.copy()

        # Build DBTable from df1 (your implementation) and measure init time
        # assume DBTable(file_name=None, from_data=df1, sort_key='value', index_levels=[1,4,8], hash_size=hash_size, depth_limit=50)
        try:
            db_table = DBTable(file_name=None, from_data=df1, sort_key='value',
                               index_levels=[1, 4, 8], hash_size=hash_size, depth_limit=50)
        except Exception as e:
            # If DBTable is not available, print a warning and skip insert timing/accuracy
            print(f"Warning: DBTable construction failed: {e}. Skipping DBTable-related timings for skew={s}.")
            insertion_times.append(None)
            init_select_time.append(None)
            insert_select_time.append(None)
            init_accuracy.append(None)
            insert_accuracy.append(None)
            continue

        # Measure insertion time (inserting insert_df rows)
        start_ts = time.time()
        for i in range(len(insert_df)):
            row = insert_df.iloc[i]
            db_table.insert(row)   # assumes db_table.insert takes a row-like object
        end_ts = time.time()
        insertion_times.append(end_ts - start_ts)

        # Evaluation accuracy timings — wrap in try/except if test_accuracy not available
        try:
            start_ts = time.time()
            accessible = test_accuracy(initial_df=initial_df, sort_key='value', table=db_table)
            end_ts = time.time()
            init_select_time.append(end_ts - start_ts)
            init_accuracy.append(accessible / len(initial_df))
        except Exception as e:
            print(f"Warning: test_accuracy on initial_df failed: {e}")
            init_select_time.append(None)
            init_accuracy.append(None)

        try:
            start_ts = time.time()
            accessible = test_accuracy(initial_df=insert_df, sort_key='value', table=db_table)
            end_ts = time.time()
            insert_select_time.append(end_ts - start_ts)
            insert_accuracy.append(accessible / len(insert_df))
        except Exception as e:
            print(f"Warning: test_accuracy on insert_df failed: {e}")
            insert_select_time.append(None)
            insert_accuracy.append(None)

    # Print summary
    print("initialization_times:", initialization_times)
    print("insertion_times:", insertion_times)
    print("init_accuracy:", init_accuracy)
    print("insert_accuracy:", insert_accuracy)
    print("init_select_time:", init_select_time)
    print("insert_select_time:", insert_select_time)

    # Your existing plot calls (assumes a `plot` helper exists)
    try:
        plot(skews, [initialization_times, insertion_times], ["Initialization", "Insertion"],
             x_label="skew", y_label="RunTime", title=title, saveas='results/skew_runtime.png')
        plot(skews, [init_accuracy, insert_accuracy], ["Initial Data", "Inserted Data"],
             x_label="skew", y_label="% of Time",
             title="Ability to Access Final Data Within Error Bounds With Changing Skew in Data",
             saveas='results/skew_accuracies.png')
        plot(skews, [init_select_time, insert_select_time], ["Initial Data", "Inserted Data"],
             x_label="skew", y_label="RunTime",
             title="RunTime For Accessing Final Data Within Error Bounds With Changing Skew in Data",
             saveas='results/skew_select_runtime.png')
    except Exception as e:
        print(f"Warning: your custom plot(...) helper failed: {e} — skipping those plots.")

    # -------------------------
    # New: Plot df_combined distributions
    # -------------------------
    # 1) For each skew, save histogram + CDF
    for s, vals in combined_by_skew.items():
        save_path = f"results/combined_hist_cdf_skew_{s}.png"
        title_str = f"Combined distribution after insertion (skew={s})"
        plot_hist_and_cdf(vals, bins=200, title=title_str, saveas=save_path, show=False)
        print(f"Saved {save_path}")

    # 2) Overlay CDFs for a subset of skews to compare distribution shift visually
    #    choose up to 5 evenly spaced skews to avoid clutter
    choose_idxs = np.linspace(0, len(skews) - 1, min(5, len(skews))).astype(int)
    overlay_dict = {skews[i]: combined_by_skew[skews[i]] for i in choose_idxs}
    overlay_path = "results/combined_overlay_cdf.png"
    plot_overlay_cdfs(overlay_dict, title="Overlayed CDFs of combined datasets (selected skews)",
                      saveas=overlay_path, show=True)
    print(f"Saved overlay CDF to {overlay_path}")

    return {
        "skews": skews,
        "initialization_times": initialization_times,
        "insertion_times": insertion_times,
        "init_accuracy": init_accuracy,
        "insert_accuracy": insert_accuracy,
        "init_select_time": init_select_time,
        "insert_select_time": insert_select_time,
        "combined_by_skew": combined_by_skew
    }

# Example usage:
# results = test_skewed_insertion()
   
        
    

if __name__ == '__main__':
    dir = 'data/original_csv/'
    filename = 'sailors'
    #test accuracy and time between None, depth =2 , depth = 10, depth = 60
    # depths = [2, 10, 50, 100, None]
    # test_depths(depths, dir=dir, filename=filename)
    
    #test runtime with smaller sizes of initial data and insert data, as well as timing
    # sizes = [.10, .20, .40, .80, 1]
    # test_sizes(sizes=sizes, dir=dir, filename=filename)
    
    #test between non uniform and uniform
    #test size of insertion
    skews = [0.5, 1.0, 1.5, 2.0]
    results = test_skewed_insertion(skews)
    print(results)